<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
            "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width">
  <title>TPDP 2025 – Theory and Practice of Differential Privacy</title>

  <link href="style.css" type="text/css" rel='stylesheet'>
  <link rel='stylesheet' media='screen and (max-width: 750px)'
  href='narrow.css'>

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<meta property="og:image" content="https://tpdp.journalprivacyconfidentiality.org/android-chrome-512x512.png">
<meta property="og:image:type" content="image/png">
<meta property="og:image:width" content="512">
<meta property="og:image:height" content="512">

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}})
</script>
</head>

<style>
  .image-with-text {
    display: flex;
    align-items: center;
  }

  .image-with-text p {
    margin-right: 10px;
  }
</style>

<body>

<div id="header">
  <h1>TPDP 2025 - Theory and Practice of Differential Privacy</h1>
  <h2>Google Mountain View - June 2-3, 2025
    <br>
  </h2>
</div>

<div class="program box">
  <h2>Workshop Information</h2>
<p>
TPDP 2025 will take place on June 2 and 3 at Google in Mountain View, CA.
</p>

<p>
  <b>Venue Information:</b> <a href="https://docs.google.com/document/d/e/2PACX-1vQMMWZhi6uXWZmWC3sfFDOh8np-VE-hgv3MTrFnpl7c0b0tXMtmCFOK0td4mPI8qUVUCJl4-nELG5D6/pub">Click here for venue information</a>, including exact location of the workshop, parking information, and check-in information.
</p>

<p>
  <b>Logistics:</b> The workshop will be held at Google Mountain View. Nearby airports include San Francisco International Airport (SFO) and San Jose International Airport (SJC). Nearby hotels include the Ameswell Hotel, Hyatt Centric Mountain View, Shashi Hotel Mountain View Palo Alto, and Aloft Mountain View.
</p>

<p>
  <b>Registration:</b> Registration is closed.
</p>

<p>
  <b>DC-Area Watch Party:</b> Christine Task at Knexus Research is hosting a DC-area Watch Party for those that can't travel to CA, but would still like to meet in person to watch presentations and discuss research. The Watch Party will take place at Knexus Research (1951 Kidwell Dr, Vienna VA 22182).  It will run 11:30am - 6pm EDT on June 2, and 11:30am - 3pm EDT on June 3rd.  Lunch is brown bag (bring your own) with snacks and coffee/tea provided, and a group dinner will be organized if there is sufficient interest. <a href="https://forms.gle/owmsXH6n73hujBDj6">Please register here for the DC-area watch party</a> by May 23. <b>Registration for the DC-Area Watch Party is separate from TPDP registration</b> (but also free) - please fill out both registration forms if you plan to attend. <a href="mailto:christine.task@knexusresearch.com">Contact Christine Task</a> with DC-specific questions.
</p>


<h2>Program</h2>

<ul>
<li>All times are <b>Pacific Daylight Time</b></li>
</ul>

<table  style="padding:15px; column-gap:1000px">

<p><b>Monday, June 2</b></p>

<tr>
<td style="width: 100px;">
8:00-9:00
</td>
<td></td>
<td>
Breakfast
<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
9:00-9:05
</td>
<td></td>
<td>
Welcome
<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
9:05-9:50
</td>
<td></td>
<td>
Keynote #1

<button type="button" class="collapsible">Practical Private Mean Estimation: Toward Instance-Adaptivity and Computational Efficiency<br>
<i>Lydia Zakynthinou (Invited Speaker)</i></button>
<div class="abstract">
  <p>Practical differentially private statistical estimation should ideally combine strong error guarantees, computational efficiency, robustness, and minimal reliance on user-specified assumptions. In this talk, I will highlight recent progress toward these goals, through the fundamental problem of mean estimation. 
  In the first part, I will present differentially private algorithms for high-dimensional mean estimation whose error optimally adapts to the effective dimensionality of the distribution. These estimators can achieve dimension-free error whenever possible—for instance, for distributions that are concentrated on a small number of principal components—overcoming a limitation of prior methods, which suffer from a curse of dimensionality and require sample sizes that scale with the ambient dimension even in these favorable regimes.

  In the second part, I will discuss recent results that uncover both limitations and new directions for designing computationally efficient estimators with affine-invariant error guarantees and robustness properties.

  This talk is primarily based on joint work with Gavin Brown, Yuval Dagan, Michael Jordan, Xuelin Yang, and Nikita Zhivotovskiy.</p> 
  <p><a href="https://lydiazakynthinou.com/">Lydia Zakynthinou's website</a></p>
</div>

<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
9:50-10:35
</td>
<td></td>
<td>
Contributed Talks: Session #1

<button type="button" class="collapsible">Leveraging Per-Instance Privacy for Machine Unlearning<br><i>Nazanin Mohammadi Sepahvand, Anvith Thudi, Berivan Isik, Ashmita Bhattacharyya, Nicolas Papernot, Eleni Triantafillou, Daniel M. Roy, Gintare Karolina Dziugaite</i></button>
<div class="abstract">
  <p>We present a principled, per-instance approach to quantifying the difficulty of unlearning via fine-tuning. We begin by sharpening an analysis of noisy gradient descent for unlearning, obtaining a better utility--unlearning tradeoff by replacing worst-case privacy loss bounds with per-instance privacy losses, each of which bounds the (Renyi) divergence to retraining without an individual data point. To demonstrate the practical applicability of our theory, we present empirical results showing that our theoretical predictions are born out both for Stochastic Gradient Langevin Dynamics (SGLD) as well as for standard fine-tuning without explicit noise. We further demonstrate that per-instance privacy losses correlate well with several existing data difficulty metrics, while also identifying harder groups of data points, and introduce novel evaluation methods based on loss barriers. All together, our findings provide a foundation for more efficient and adaptive unlearning strategies tailored to the unique properties of individual data points.</p>
</div>

<button type="button" class="collapsible">Privacy amplification by random allocation<br><i><b>Moshe Shenfeld</b>, Vitaly Feldman</i></button>
<div class="abstract">
  <p>We consider the privacy privacy amplification properties of a sampling scheme in which a user's data is used in $k$ steps chosen randomly and uniformly from a sequence (or set) of $t$ steps. This sampling scheme has been recently applied in the context of differentially private optimization [Chua et al., 2024a, Choquette-Choo et al., 2024] and is also motivated by communication-efficient high-dimensional private aggregation [Asi et al., 2025]. Existing analyses of this scheme either rely on privacy amplification by shuffling which leads to overly conservative bounds or require Monte Carlo simulations that are computationally prohibitive in most practical scenarios.</p>
</div>

<button type="button" class="collapsible">Generate-then-Verify: Reconstructing Data from Limited Published Statistics<br><i>Terrance Liu, Eileen Xiao, Pratiksha Thaker, Adam Smith, Zhiwei Steven Wu</i></button>
<div class="abstract">
  <p>We study the problem of reconstructing tabular data from aggregate statistics, in which the attacker aims to identify interesting claims about the sensitive data that can be verified with 100% certainty given the aggregates. Successful attempts in prior work have conducted studies in settings where the set of published statistics is rich enough that entire datasets can be reconstructed with certainty. In our work, we instead focus on the regime where many possible datasets match the published statistics, making it impossible to reconstruct the entire private dataset perfectly (i.e., when approaches in prior work fail). We propose the problem of partial data reconstruction, in which the goal of the adversary is to instead output a subset of rows and/or columns that are guaranteed to be correct. We introduce a novel integer programming approach that first generates a set of claims and then verifies whether each claim holds for all possible datasets consistent with the published aggregates. We evaluate our approach on the housing-level microdata from the U.S. Decennial Census release, demonstrating that privacy violations can still persist even when information published about such data is relatively sparse.</p>
</div>

<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
10:35-11:00
</td>
<td></td>
<td>
Break<br>
<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
11:00-12:30
</td>
<td></td>
<td>
<a href="#poster1">Poster Session #1</a>
<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
12:30-1:30
</td>
<td></td>
<td>
Lunch (provided)<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
1:30-2:30
</td>
<td></td>
<td>
Panel Discussion: Using DP Data<br>
<i>Panelists: Jörg Drechsler (Institute for Employment Research), Stefano Iacus (Harvard), Harikesh Nair (Google)</i><br>
<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
2:30-2:50
</td>
<td></td>
<td>
Break<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
2:50-3:50
</td>
<td></td>
<td>
Contributed Talks: Session #2

<button type="button" class="collapsible">The Last Iterate Advantage: Empirical Auditing and Principled Heuristic Analysis of Differentially Private SGD <br><i><b>Thomas Steinke</b>, Milad Nasr, Arun Ganesh, Borja Balle, Christopher A. Choquette-Choo, Matthew Jagielski, Jamie Hayes, Abhradeep Guha Thakurta, Adam Smith, Andreas Terzis</i></button>
<div class="abstract">
  <p>We propose a simple heuristic privacy analysis of noisy clipped stochastic gradient descent (DP-SGD) in the setting where only the last iterate is released and the intermediate iterates remain hidden. Namely, our heuristic assumes a linear structure for the model.
We show experimentally that our heuristic is predictive of the outcome of privacy auditing applied to various training procedures. Thus it can be used prior to training as a rough estimate of the final privacy leakage. We also probe the limitations of our heuristic by providing some artificial counterexamples where it underestimates the privacy leakage.
The standard composition-based privacy analysis of DP-SGD effectively assumes that the adversary has access to all intermediate iterates, which is often unrealistic. However, this analysis remains the state of the art in practice. While our heuristic does not replace a rigorous privacy analysis, it illustrates the large gap between the best theoretical upper bounds and the privacy auditing lower bounds and sets a target for further work to improve the theoretical privacy analyses. We also empirically support our heuristic and show existing privacy auditing attacks are bounded by our heuristic analysis in both vision and language tasks.</p>
</div>

<button type="button" class="collapsible">Trade-offs in Data Memorization via Strong Data Processing Inequalities <br><i>Vitaly Feldman, Guy Kornowski, <b>Xin Lyu</b></i></button>
<div class="abstract">
  <p>Recent research demonstrated that training large language models involves memorization of a significant fraction of training data. Such memorization can lead to privacy violations when training on sensitive user data and thus motivates the study of data memorization's role in learning. In this work, we demonstrate that several simple and well-studied binary classification problems exhibit a trade-off between the number of samples available to a learning algorithm and the amount of information about the training data that a learning algorithm needs to memorize to be accurate.

In particular, $\Omega(d)$ bits of information about the training data need to be memorized when a single $d$-dimensional example is available, which then decays as $\Theta(d/n)$ as the number of examples grows (for $n\leq \sqrt{d}$). Further,  this rate is achieved (up to logarithmic factors) by simple learning algorithms. Our results build on the work of Brown et al. (2021) and establish a new framework for proving memorization lower bounds that is based on an approximate version of strong data processing inequalities.
</p>
</div>

<button type="button" class="collapsible">New Bounds for Private Graph Optimization Problems via Synthetic Graphs<br><i>Anders Aamand, Rasmus Pagh, <b>Lukas Retschmeier</b></i></button>
<div class="abstract">
  <p>We consider the graph optimization problems of privately releasing the edges of minimum-weight \emph{spanning trees}, \emph{perfect matchings}, and \emph{shortest paths} as introduced by Sealfon [PODS 2016].</p>

<p>Given a public graph topology $G=(V, E)$, together with private edge weights $\mathbf{W} \in \mathbb{R}^{E}$, we want to publish an approximate solution to those problems under \emph{edge-weight} differential privacy.
We show new asymptotically tight additive error bounds for all three problems under the $\ell_1$ neighboring relationship.Interestingly, the mechanisms achieving these bounds are the simplest one can imagine: Construct a private synthetic graph by adding noise to the edge weights and then run a non-private graph algorithm. Concretely, we improve Sealfon's lower bound technique for spanning trees and matchings, increasing the bound from $\Omega(n/\epsilon)$ to $\Omega((n/\epsilon) \min(\log(1/\delta), \log n))$ which matches the known upper bound in the regime $\delta=n^{-\Omega(1)}$.</p>

<p>For shortest paths, we show that adding Laplace noise to weights and capping the noisy edge weight at zero (so that the new weight is non-negative), yields a synthetic graph that, with probability $1-\exp(-\Omega(n))$, preserves all $s$-$t$ shortest path distances within additive error $O(n/\eps)$. This improves on Sealfon's $O((n\log n)/\eps)$ upper bound and matches his lower bound within a constant factor. </p>

<p>Lastly, we present preliminary work on these problems under the $\ell_\infty$ neighboring relationship.</p>
</div>

<button type="button" class="collapsible">Fully Dynamic Graph Algorithms with Edge Differential Privacy<br><i><b>Sofya Raskhodnikova</b>, Teresa Anna Steiner</i></button>
<div class="abstract">
  <p>We study differentially private algorithms for analyzing graphs in the challenging setting of continual release with fully dynamic updates, where edges are inserted and deleted over time, and the algorithm is required to update the solution at every time step. Previous work has presented differentially private algorithms for many graph problems that can handle insertions only or deletions only (called partially dynamic algorithms) and obtained some hardness results for the fully dynamic setting. The only algorithms in the latter setting were for the edge count, given by Fichtenberger, Henzinger, and Ost (ESA 21), and for releasing the values of all graph cuts, given by Fichtenberger, Henzinger, and Upadhyay (ICML 23). We provide the first differentially private and fully dynamic graph algorithms for several other fundamental graph statistics (including the triangle count, the number of connected components, the size of the maximum matching, and the degree histogram), analyze their error and show strong lower bounds on the error for all algorithms in this setting. We study two variants of edge differential privacy for fully dynamic graph algorithms: event-level and item-level. We give upper and lower bounds on the error of both event-level and item-level fully dynamic algorithms for several fundamental graph problems. No fully dynamic algorithms that are private at the item-level (the more stringent of the two notions) were known before. In the case of item-level privacy, for several problems, our algorithms match our lower bounds.</p>
</div>

<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
3:50-5:20
</td>
<td></td>
<td>
<a href="#poster2">Poster Session #2</a>
<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
5:20-7:00
</td>
<td></td>
<td>
Reception
<br>
</td>
</tr>

</table>

<br>
<table  style="padding:15px; column-gap:1000px">
<p><b>Tuesday, June 3</b></p>

<tr>
<td style="width: 100px;">
8:00-9:00
</td>
<td></td>
<td>
Breakfast
<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
9:00-9:50
</td>
<td></td>
<td> Keynote #2

<button type="button" class="collapsible">Let's Ask Some Big Questions About Theory and Practice.<br>
<i>Lucas Rosenblatt (Invited Speaker)</i></button>
<div class="abstract">
  <p>In this talk I try to stitch together some recent work - some my own and some others’ - to probe practical, important, and intertwined questions: How can we provide and evaluate differentially private (DP) synthetic data to address epistemic concerns of practitioners? Are we targeting the problems practitioners face every day - say, DP under extreme class imbalance, for e.g., EHRs and fraud detection - or merely the ones that make tidy papers? With the advent of LLM supremacy, how do we want to treat its output (Is it public? Is it useful under DP?) Do we understand the right LLM threat models, and do DP defenses make sense? I will trace these questions from theory to practice as best I can, and frame open challenges that I hope we can address as a community.</p>
  <p><a href="https://lucasrosenblatt.com/">Lucas Rosenblatt's website</a></p>
</div>

<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
9:50-10:35
</td>
<td></td>
<td>
Contributed Talks: Session #3

<button type="button" class="collapsible">Differentially Private Learning Beyond the Classical Dimensionality Regime <br><i>Cynthia Dwork, <b>Pranay Tankala</b>, Linjun Zhang</i></button>
<div class="abstract">
  <p>We initiate the study of differentially private learning in the proportional dimensionality regime, in which the number of data samples n and problem dimension d approach infinity at rates proportional to one another, meaning that d/n-->delta as n-->infinity for an arbitrary, given constant 0<$delta$<$infinity$. This setting is significantly more challenging than that of all prior theoretical work in high-dimensional differentially private learning, which, despite the name, has assumed that delta=0 or is sufficiently small for problems of sample complexity O(d), a regime typically considered “low-dimensional” or “classical” by modern standards in high-dimensional statistics.</p>
</div>

<button type="button" class="collapsible">Adapting to Linear Separable Subsets with Large-Margin in Differentially Private Learning
<br><i><b>Erchi Wang</b>, Yuqing Zhu, Yu-Xiang Wang</i></button>
<div class="abstract">
  <p>This paper studies the problem of differentially private empirical risk minimization (DP-ERM) for binary linear classification. We obtain an efficient $(\varepsilon,\delta)$-DP algorithm with an empirical zero-one risk bound of $\tilde{O}\left(\frac{1}{\gamma^2\varepsilon n} + \frac{|S_{\mathrm{out}}|}{\gamma n}\right)$ where $n$ is the number of data points, $S_{\mathrm{out}}$ is an arbitrary subset of data one can remove and $\gamma$ is the margin of linear separation of the remaining data points (after $S_{\mathrm{out}}$ is removed). Here, $\tilde{O}(\cdot)$ hides only logarithmic terms. In the agnostic case, we improve the existing results when the number of outliers is small. Our algorithm is highly adaptive because it does not require knowing the margin parameter $\gamma$ or outlier subset $S_{\mathrm{out}}$.</p>
</div>

<button type="button" class="collapsible">It's My Data Too: Private ML for Datasets with Multi-User Training Examples<br><i><b>Arun Ganesh</b>, Ryan McKenna, Brendan McMahan, Adam Smith, Fan Wu</i></button>
<div class="abstract">
  <p>We initiate a study of algorithms for model training with user-level differential privacy (DP), where each example may be attributed to multiple users, which we call the multi-attribution model. We first provide a carefully chosen definition of user-level DP under the multi-attribution model. Training in the multi-attribution model is facilitated by solving the contribution bounding problem, i.e. the problem of selecting a subset of the dataset for which each user is associated with a limited number of examples. We propose a greedy baseline algorithm for the contribution bounding problem. We then empirically study this algorithm for a synthetic logistic regression task and a transformer training task, including studying variants of this baseline algorithm that optimize the subset chosen using different techniques and criteria. We find that the baseline algorithm remains competitive with its variants in most settings, and build a better understanding of the practical importance of a bias-variance tradeoff inherent in solutions to the contribution bounding problem.</p>
</div>

<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
10:35-11:00
</td>
<td></td>
<td>
Break<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
11:00-12:30
</td>
<td></td>
<td>
Panel Discussion: DP Law and Policy
<i>Panelists: Ryan Steed (Princeton), Mayana Pereira (Capital One), Nitin Kohli (UC Berkeley), Alex Wood (Harvard)</i><br>
<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
12:30-1:30
</td>
<td></td>
<td>
Lunch (provided)<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
1:30-2:30
</td>
<td></td>
<td>
Contributed Talks: Session #4

<button type="button" class="collapsible">Laplace Transform Interpretation of Differential Privacy<br><i><b>Rishav Chourasia</b>, Uzair Javaid, Biplap Sikdar</i></button>
<div class="abstract">
  <p>Differential Privacy has become the gold standard for quantifying privacy in data analysis and machine learning algorithms. Thanks to nearly two decades of research, there are now multiple competing notions of differential privacy—(ε,δ)-DP, Rényi DP, and Privacy Loss Distribution (PLD) tail—each with its own advantages and limitations. In this talk, I will describe how all these notions can be viewed as primal and dual formulations of each other through Laplace transforms. Notably, the Laplace transform expressions of DP provides an elegant framework for reasoning about DP and its properties. To showcase this perspective I will introduce a new composition theorem for (ε,δ)-DP that is tight—even in the constants—and improves upon the result of Kairouz et al. (2015), which was previously considered optimal.</p>
</div>

<button type="button" class="collapsible">Fingerprinting Codes Meet Geometry: Improved Lower Bounds for Private Query Release and Adaptive Data Analysis<br><i><b>Xin Lyu</b>, Kunal Talwar</i></button>
<div class="abstract">
  <p>We further explore applications of the fingerprinting method in private and adaptive data analysis. By combining the exponential family with a proper version of Stoke's theorem, we develop a simple and versatile fingerprinting framework, and prove several new results:

First, we show a sample complexity lower bound of $\widetilde{Omega(\log(Q) \sqrt(\log(N)) / \alpha^3)$ for adaptive data analysis on linear queries, assuming the algorithm is both distrbutional and emprically accurate. Up to the ``emprically accurate'' caveat, this is nearly tight and matches the upper bound from [BNSSSU'16].

Secondly, we characterize the sample complexity for answering a workload of $Q$ random counting queries over a universe of size $N$, in both ``high accuracy'' and ``low accuracy'' regimes.

Thirdly, we revisit the [BUV'14] sample complexity lower bound for answering a worst-case ensemble of counting queries, and improve the lower bound therein by a $\sqrt{\log(1/\delta)}$ factor, thereby achieving a tight-up-to-constant lower bound.
</p>
</div>

<button type="button" class="collapsible">Instance-Optimality for Private KL Distribution Estimation<br><i><b>Jiayuan Ye</b>, Vitaly Feldman, Kunal Talwar</i></button>
<div class="abstract">
  <p>We study the fundamental problem of estimating an unknown discrete distribution $p$ over $d$ symbols, given $n$ i.i.d. samples from the distribution. We are interested in minimizing the KL divergence between the true distribution and the algorithm's estimate. We first show that the standard minimax objective is too ``worst-case'' to shed light on optimality of algorithms for this problem, where we prove that simple DP estimators with poor empirical performance are already minimax optimal. This is because minimax objective focuses on minimizing the estimation error on the worst-case data distribution, and fails to shed light on an algorithm's performance on individual (non-worst-case) instances $p$. Thus, we study this problem from an instance-optimality viewpoint, where the algorithm's error on $p$ is compared to the minimum achievable estimation error over a small local neighborhood of $p$. Under natural notions of local neighborhood, we propose algorithms that achieve instance-optimality up to constant factors, with and without a differential privacy constraint. Our upper bounds rely on (private) variants of the Good-Turing estimator. Our lower bounds use additive local neighborhoods that more precisely captures the hardness of distribution estimation in KL divergence, compared to the permutation and multiplicative neighborhoods considered in prior works.</p>
</div>

<button type="button" class="collapsible">Purifying Approximate Differential Privacy with Randomized Post-processing <br><i><b>Yingyu Lin, Erchi Wang</b>, Yi-An Ma, Yu-Xiang Wang</i></button>
<div class="abstract">
  <p>We propose a framework to convert $(\varepsilon, \delta)$-approximate Differential Privacy (DP) mechanisms into $(\varepsilon', 0)$-pure DP mechanisms under certain conditions, a process we call ``purification.'' This algorithmic technique leverages randomized post-processing with calibrated noise to eliminate the $\delta$ parameter while achieving near-optimal privacy-utility tradeoff for pure DP. It enables a new design strategy for pure DP algorithms: first run an approximate DP algorithm with certain conditions, and then purify.
This approach allows one to leverage techniques such as strong composition and propose-test-release that require $\delta>0$ in designing pure-DP methods with $\delta=0$. We apply this framework in various settings, including Differentially Private Empirical Risk Minimization (DP-ERM), stability-based release, and query release tasks. To the best of our knowledge, this is the first work with a statistically and computationally efficient reduction from approximate DP to pure DP. Finally, we illustrate the use of this reduction for proving lower bounds under approximate DP constraints with explicit dependence in $\delta$, avoiding the sophisticated fingerprinting code construction.
https://arxiv.org/abs/2503.21071
</p>
</div>




<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
2:30-4:00
</td>
<td></td>
<td>
<a href="#poster3">Poster Session #3</a>
<br>
</td>
</tr>

<tr>
<td style="width: 100px;">
4:00-4:45
</td>
<td></td>
<td>
Contributed Talks: Session #5

<button type="button" class="collapsible">Mayfly: Private Aggregate Insights from Ephemeral Streams of On-Device User Data<br><i>Christopher Bian, Albert Cheu, Stanislas Chiknavaryan, Zoe Gong, Marco Gruteser, Oliver Guinan, Yannis Guzman, Peter Kairouz, Artem Lagzdin, Ryan McKenna, Grace Ni, <b>Edo Roth</b>, Maya Spivak, Timon Van Overveldt, Ren Yi</i></button>
<div class="abstract">
  <p>This work introduces Mayfly, a federated analytics approach enabling aggregate queries over ephemeral on-device data streams without central persistence of sensitive user data. Mayfly minimizes data via on-device windowing and contribution bounding through SQL-programmability, anonymizes user data via streaming differential privacy (DP), and mandates immediate in-memory cross-device aggregation on the server -- ensuring only privatized aggregates are revealed to data analysts. Deployed for a sustainability use case estimating transportation carbon emissions from private location data, Mayfly computed over 4 million statistics across more than 500 million devices with a per-device, per-week DP ε=2 while meeting strict data utility requirements. To achieve this, we designed a new DP mechanism for Group-By-Sum workloads leveraging statistical properties of location data, with potential applicability to other domains.</p>
</div>

<button type="button" class="collapsible">Mildly Accurate Computationally Differentially Private Inner Product Protocols Imply Oblivious Transfer<br><i>Iftach Haitner, Noam Mazor, Jad Silbak, Eliad Tsfadia, Chao Yan</i></button>
<div class="abstract">
  <p>In distributed differential privacy, multiple parties collaboratively analyze their combined data while protecting the privacy of each party's data from the eyes of the others. Interestingly, for certain fundamental two-party functions like inner product and Hamming distance, the accuracy of distributed solutions significantly lags behind what can be achieved in the centralized model. However, under computational differential privacy, these limitations can be circumvented using oblivious transfer via secure multi-party computation. Yet, no results show that oblivious transfer is indeed necessary for accurately estimating a non-Boolean functionality.  In particular, for the inner-product functionality, it was previously unknown whether oblivious transfer is necessary even for the best possible constant additive error.

  In this work, we prove that any computationally differentially private protocol that estimates the inner product over {-1,1}^n x {-1,1}^n up to an additive error of O(n^{1/6}), can be used to construct oblivious transfer. In particular, our result implies that protocols with sub-polynomial accuracy are equivalent to oblivious transfer. In this accuracy regime, our result improves upon Haitner, Mazor, Silbak, and Tsfadia [STOC '22] who showed that a key-agreement protocol is necessary.</p>
</div>

<button type="button" class="collapsible">Enforcing Demographic Coherence: A Harms Aware Framework for Reasoning about Private Data Release<br><i>Mark Bun, Marco Carmosino, <b>Palak Jain</b>, Gabriel Kaptchuk, Satchit Sivakumar</i></button>
<div class="abstract">
  <p>The technical literature about data privacy largely consists of two complementary approaches: formal definitions of conditions sufficient for privacy preservation and attacks that demonstrate privacy breaches. Differential privacy is an accepted standard in the former sphere.  However, differential privacy's powerful adversarial model and worst-case guarantees may make it too stringent in some situations, especially when achieving it comes at a significant cost to data utility. Meanwhile, privacy attacks aim to expose real and worrying privacy risks associated with existing data aggregation systems, but do not identify what properties are necessary to defend against them.</p>
</div>


<br>
</td>
</tr>


</table>
<br>
</div>

<div class="content box"> <h2>Accepted Papers</h2>

<a name="poster1"></a>
<p><b>Poster Session 1 </b></p>
<ul>
<li> <a href="pdf/hiraoka.pdf">A Programming Framework for Estimating Privacy Loss in Differential Privacy</a> <br> <i> Takumi Hiraoka, Shumpei Shiina, Kenjiro Taura, Takumi Hiraoka </i> </li> <br>
<li> Differentially Private Winnow for Learning Halfspaces and Decision Lists <br> <i> Mark Bun, William Fang </i> </li> <br>
<li> Empirical Improvements for Differentially Private Subspace Recovery <br> <i> Vikrant Singhal, Roy Rinberg, Eran Malach, Seth Neel, Salil Vadhan </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2503.03043">Leveraging Randomness in Model and Data Partitioning for Privacy Amplification</a> <br> <i> Andy Dong, Wei-Ning Chen, Ayfer Ozgur </i> </li> <br>
<li> Laplace Transform Interpretation of Differential Privacy <br> <i> Rishav Chourasia, Uzair Javaid, Biplap Sikdar </i> </li> <br>
<li> <a href="https://arxiv.org/pdf/2504.14061">Benchmarking Differentially Private Tabular Data Synthesis Algorithms</a> <br> <i> Kai Chen, Xiaochen Li, Chen Gong, Ryan McKenna, Tianhao Wang </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2503.12008">Winning the MIDST Challenge: New Membership Inference Attacks on Diffusion Models for Tabular Data Synthesis</a> <br> <i> Xiaoyu Wu, Yifei Pang, Terrance Liu, Steven Wu </i> </li> <br>
<li> <a href="https://arxiv.org/pdf/2412.07962">Mayfly: Private Aggregate Insights from Ephemeral Streams of On-Device User Data</a> <br> <i> Christopher Bian, Albert Cheu, Stanislas Chiknavaryan, Zoe Gong, Marco Gruteser, Oliver Guinan, Yannis Guzman, Peter Kairouz, Artem Lagzdin, Ryan McKenna, Grace Ni, Edo Roth, Maya Spivak, Timon Van Overveldt, Ren Yi </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2402.08156">Differentially Private Distributed Inference</a> <br> <i> Marios Papachristo, Amin Rahimian </i> </li> <br>
<li> <a href="https://arxiv.org/pdf/2402.09483">Oracle-Efficient Differentially Private Learning with Public Data</a> <br> <i> Rathin Desai, Mark Bun, Adam Block, Zhiwei Steven Wu, Abhishek Shetty </i> </li> <br>
<li> Faster Rates for Private Adversarial Bandits <br> <i> Vinod Raman, Kunal Talwar, Hilal Asi </i> </li> <br>
<li> <a href="https://xingyuzhou.org/publications/TPDP-square.pdf">SquareχPO: Differentially Private and Robust χ2-Preference Optimization in Offline Direct Alignment</a> <br> <i> Xingyu Zhou, Yulian Wu, Wenqian Weng, Francesco Orabona </i> </li> <br>
<li> <a href="https://proceedings.mlr.press/v272/cheu25a.html">Differentially Private Multi-Sampling from Distributions</a> <br> <i> Albert Cheu, Debanuj Nayak, Debanuj Nayak </i> </li> <br>
<li> <a href="pdf/kaiser.pdf">User-Level Differential Privacy in Medical Machine Learning</a> <br> <i> Johannes Kaiser, Jakob Eigenmann, Daniel Rueckert, Georgios Kaissis </i> </li> <br>
<li> Continuous Private Release of Sparse Histograms <br> <i> Edith Cohen, Vadym Doroshenko, Badih Ghazi, Pritish Kamath, Alexander Knop, Ravi Kumar, Pasin Manurangsi, Ethan Leeman, Ishika Mitra, Adam Sealfon </i> </li> <br>
<li> <a href="pdf/ligett.pdf">Differentially Private Non-Parametric Confidence Intervals</a> <br> <i> Tomer Shoham, Moshe Shenfeld, Katrina Ligett, Noa Velner-Harris </i> </li> <br>
<li> <a href="pdf/wang.pdf">Integrating Feature Correlation in Differential Privacy with Applications in DP-ERM</a> <br> <i> Tianyu Wang, Luhao Zhang, Rachel Cummings </i> </li> <br>
<li> Finding the Right Fit: Differentially Private Network Data Release At Scale <br> <i> Vasanta Chaganti, Ziming Yuan, Amelia Meles, Xinxin Li </i> </li> <br>
<li> It's My Data Too: Private ML for Datasets with Multi-User Training Examples <br> <i> Arun Ganesh, Ryan McKenna, Brendan McMahan, Adam Smith, Fan Wu </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2504.21178">Differentially Private Secure Multiplication with Erasures and Adversaries</a> <br> <i> Haoyang Hu, Viveck R. Cadambe </i> </li> <br>
<li> Controlling the spread of Epidemics on Networks with Differential Privacy <br> <i> Dung Nguyen, Aravind Srinivasan, Renata Valieva, Anil Vullikanti, Jiayi Wu </i> </li> <br>
<li> <a href="https://www.arxiv.org/pdf/2504.19001">Differentially Private Quasi-Concave Optimization: Bypassing the Lower Bound and Application to Geometric Problems</a> <br> <i> Kobbi Nissim, Eliad Tsfadia, Chao Yan </i> </li> <br>
<li> <a href="Will be on arXiv next week. Will send the link to the chair then.">Private Lossless Multiple Release</a> <br> <i> Joel Daniel Andersson, Rasmus Pagh, Boel Nelson, Lukas Retschmeier </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2409.19800">Differentially Private Bilevel Optimization</a> <br> <i> Guy Kornowski </i> </li> <br>
<li> <a href="https://arxiv.org/pdf/2411.14639">Differentially Private Adaptation of Diffusion Models via Noisy Aggregated Embeddings</a> <br> <i> Pura Peetathawatchai, Wei-Ning Chen, Berivan Isik, Sanmi Koyejo, Albert No </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2505.20351">Differential Privacy and Ratio Statistics</a> <br> <i> Tomer Shoham, Katrina Ligett </i> </li> <br>
<li> Differentially Private Space-Efficient Algorithms for Counting Distinct Elements in the Turnstile Model <br> <i> Rachel Cummings, Alessandro Epasto, Jieming Mao, Tamalika Mukherjee, Tingting Ou, Peilin Zhong </i> </li> <br>
<li> Adapting to Linear Separable Subsets with Large-Margin in Differentially Private Learning <br> <i> Erchi Wang, Yuqing Zhu, Yu-Xiang Wang </i> </li> <br>
<li> Retraining with Predicted Hard Labels Provably Increases Model Accuracy <br> <i> Rudrajit Das, Inderjit S Dhillon, Alessandro Epasto, Adel Javanmard, Jieming Mao, Vahab Mirrokni, Sujay Sanghavi, Peilin Zhong </i> </li> <br>
<li> <a href="pdf/kumar.pdf">Private Geometric Median in Nearly-Linear Time</a> <br> <i> Syamantak Kumar, Daogao Liu, Kevin Tian, Chutong Yang </i> </li> <br>
<li> <a href="https://cs.uwaterloo.ca/~s693zhan/papers/TPDP/2025/TPDP_2025.pdf">FedDPSyn: Federated Tabular Data Synthesis with Computational Differential Privacy</a> <br> <i> Shufan Zhang, Haochen Sun, Karl Knopf, Shubhankar Mohapatra, Wei Pang, Calvin Wang, Yingke Wang, Masoumeh Shafieinejad, David Emerson, Xi He </i> </li> <br>
<li> Understanding Private Learning From Feature Perspective <br> <i> Meng Ding, Mingxi Lei, Shaopeng Fu, Di Wang, Jinhui Xu </i> </li> <br>
<li> <a href="https://www.arxiv.org/abs/2504.21199">Generate-then-Verify: Reconstructing Data from Limited Published Statistics</a> <br> <i> Terrance Liu, Eileen Xiao, Pratiksha Thaker, Adam Smith, Zhiwei Steven Wu </i> </li> <br>
<li> <a href="https://eprint.iacr.org/2025/817">Relating Definitions of Computational Differential Privacy in Wider Parameter Regimes</a> <br> <i> Fredrik Meisingseth, Christian Rechberger </i> </li> <br>
<li> <a href="-1">Unbounded Differential Privacy Secure Against Timing Attacks</a> <br> <i> Zachary Ratliff, Salil Vadhan </i> </li> <br>
<li> (ε, δ) Considered Harmful – Best Practices for Reporting Differential Privacy Guarantees <br> <i> Juan Felipe Gomez, Bogdan Kulynych, Georgios Kaissis, Jamie Hayes, Borja Balle, Antti Honkela </i> </li> <br>
<li> Differential Privacy via Gaussian Mixing Mechanisms <br> <i> Omri Lev, Ayush Sekhari, Ashia Wilson </i> </li> <br>
<li> <a href="https://arxiv.org/pdf/2503.24321">Sample-Optimal Private Regression in Polynomial Time</a> <br> <i> Prashanti Anderson, Ainesh Bakshi, Stefan Tiegel, Mahbod Majid </i> </li> <br>
<li> <a href="https://www.pnas.org/doi/10.1073/pnas.2423072122">Privacy for Free in the Over-Parameterized Regime</a> <br> <i> Simone Bombari, Marco Mondelli </i> </li> <br>
<li> <a href="">Lower Bounds for Public-Private Computation under Distribution Shift</a> <br> <i> Amrith Setlur, Pratiksha Thaker, Jonathan Ullman </i> </li> <br>
</ul>


<a name="poster2"></a>
<p><b>Poster Session 2</b></p>
<ul>
<li> <a href="https://arxiv.org/abs/2405.20769">Avoiding Pitfalls for Privacy Accounting of Subsampled Mechanisms under Composition</a> <br> <i> Christian Janos Lebeda, Matthew Regehr, Gautam Kamath, Thomas Steinke </i> </li> <br>
<li> "We Need a Standard”: Toward an Expert–Informed Privacy Label for Differential Privacy <br> <i> Onyinye Dibia, Mengyi Lu, Prianka Bhattacharjee, Yuanyuan Feng, Joseph P. Near </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2503.01766">Optimal Differentially Private Sampling of Unbounded Gaussians</a> <br> <i> Valentio Iverson, Gautam Kamath, Argyris Mouzakis </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2406.04827">Auditing Differential Privacy Guarantees Using Density Estimation</a> <br> <i> Antti Koskela, Jafar Mohammadi </i> </li> <br>
<li> Differentially Private Gomory-Hu Trees <br> <i> Anders Aamand, Justin Chen, Mina Dalirrooyfard, Slobodan Mitrovic, Yuriy Nevmyvaka, Sandeep Silwal, Yinzhan Xu </i> </li> <br>
<li> Do You Really Need Public Data? Surrogate Public Data for Differential Privacy on Tabular Data <br> <i> Shlomi Hod, Lucas Rosenblatt, Julia Stoyanovich </i> </li> <br>
<li> <a href="https://eprint.iacr.org/2025/805">Accelerating Multiparty Noise Generation Using Lookups</a> <br> <i> Fredrik Meisingseth, Christian Rechberger, Fabian Schmid </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2501.18532">Differentially Private Steering for Large Language Model Alignment</a> <br> <i> Anmol Goel, Yaxi Hu, Iryna Gurevych, Amartya Sanyal </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2501.18914">Scaling Laws for Differentially Private Language Models</a> <br> <i> Ryan McKenna, Yangsibo Huang, Amer Sinha, Borja Balle, Zachary Charles, Christopher A. Choquette-Choo, Badih Ghazi, George Kaissis, Ravi Kumar, Ruibo Liu, Da Yu, Chiyuan Zhang </i> </li> <br>
<li> Fingerprinting Codes Meet Geometry: Improved Lower Bounds for Private Query Release and Adaptive Data Analysis <br> <i> Xin Lyu, Kunal Talwar </i> </li> <br>
<li> Agnostic Private Density Estimation for GMMs via List Global Stability <br> <i> Mohammad Afzali, Hassan Ashtiani, Christopher Liaw </i> </li> <br>
<li> <a href="pdf/sepahvand.pdf">Leveraging Per-Instance Privacy for Machine Unlearning</a> <br> <i> Nazanin Mohammadi Sepahvand, Anvith Thudi, Berivan Isik, Ashmita Bhattacharyya, Nicolas Papernot, Eleni Triantafillou, Daniel M. Roy, Gintare Karolina Dziugaite </i> </li> <br>
<li> <a href="https://arxiv.org/pdf/2502.08202">Privacy amplification by random allocation</a> <br> <i> Moshe Shenfeld, Vitaly Feldman </i> </li> <br>
<li> <a href="pdf/campbell.pdf">Decentralized Differentially Private Power Method</a> <br> <i> Andrew Campbell, Anna Scaglione, Sean Peisert </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2503.21071">Purifying Approximate Differential Privacy with Randomized Post-processing</a> <br> <i> Yingyu Lin, Erchi Wang, Yi-An Ma, Yu-Xiang Wang </i> </li> <br>
<li> <a href="pdf/shiina.pdf">PrivJail: Enforcing Differential Privacy in Pythonic Data Processing</a> <br> <i> Shumpei Shiina, Sho Nakatani, Takumi Hiraoka, Kenjiro Taura, Shumpei Shiina </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2501.18121">Optimal Survey Design for Private Mean Estimation</a> <br> <i> Yu-Wei Chen, Jordan Awan, Raghu Pasupathy </i> </li> <br>
<li> <a href="https://arxiv.org/pdf/2502.15629">Mildly Accurate Computationally Differentially Private Inner Product Protocols Imply Oblivious Transfer</a> <br> <i> Iftach Haitner, Noam Mazor, Jad Silbak, Eliad Tsfadia, Chao Yan </i> </li> <br>
<li> <a href="pdf/liu.pdf">Enhancing One-run Privacy Auditing with Quantile Regression-Based Membership Inference</a> <br> <i> Terrance Liu, Matteo Boglioni, Yiwei Fu, Shengyuan Hu, Pratiksha Thaker, Zhiwei Steven Wu </i> </li> <br>
<li> <a href="">Online Factorization Mechanisms for Queries with Bounded VC Dimension</a> <br> <i> Aleksandar Nikolov, Haohua Tang, Aleksandar Nikolov </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2503.12314">Empirical Privacy Variance</a> <br> <i> Yuzheng Hu, Fan Wu, Ruicheng Xian, Yuhang Liu, Lydia Zakynthinou, Pritish Kamath, Chiyuan Zhang, David Forsyth </i> </li> <br>
<li> Instance-Optimality for Private KL Distribution Estimation <br> <i> Jiayuan Ye, Vitaly Feldman, Kunal Talwar </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2405.16663">Private Edge Density Estimation for Random Graphs: Optimal, Efficient and Robust</a> <br> <i> Hongjie Chen, Jingqiu Ding, Yiding Hua, David Steurer </i> </li> <br>
<li> <a href="https://arxiv.org/pdf/2412.00497">Distributed Differentially Private Data Analytics via Secure Sketching</a> <br> <i> Jakob Burkhardt, Hannah Keller, Claudio Orlandi, Chris Schwiegelshohn </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2502.02709">Enforcing Demographic Coherence: A Harms Aware Framework for Reasoning about Private Data Release</a> <br> <i> Mark Bun, Marco Carmosino, Palak Jain, Gabriel Kaptchuk, Satchit Sivakumar </i> </li> <br>
<li> <a href="https://xingyuzhou.org/publications/TPDP-PG.pdf ">On the Sample Complexity of Differentially Private Policy Gradient</a> <br> <i> Yi He, Xingyu Zhou </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2502.06555">Is API Access to LLMs Useful for Generating Private SyntheticTabular Data?</a> <br> <i> Marika Swanberg, Ryan McKenna, Edo Roth, Albert Cheu, Peter Kairouz </i> </li> <br>
<li> <a href="https://www.arxiv.org/abs/2407.12108">Private prediction for large-scale synthetic text generation</a> <br> <i> Kareem Amin, Alex Bie, Weiwei Kong, Alexey Kurakin, Natalia Ponomareva, Umar Syed, Andreas Terzis, Sergei Vassilvitskii </i> </li> <br>
<li> Differential Privacy for Connectedness Indices <br> <i> Tom Rutter, Amin Rahimian </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2412.12374">Privacy in Metalearning and Multitask Learning: Modeling and Separations</a> <br> <i> Maryam Aliakbarpour, Konstantina Bairaktari, Adam Smith, Marika Swanberg, Jonathan Ullman </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2410.06186">The Last Iterate Advantage: Empirical Auditing and\\Principled Heuristic Analysis of Differentially Private SGD</a> <br> <i> Thomas Steinke, Milad Nasr, Arun Ganesh, Borja Balle, Christopher A. Choquette-Choo, Matthew Jagielski, Jamie Hayes, Abhradeep Guha Thakurta, Adam Smith, Andreas Terzis </i> </li> <br>
<li> <a href="pdf/aamand.pdf">New Bounds for Private Graph Optimization Problems via Synthetic Graphs</a> <br> <i> Anders Aamand, Rasmus Pagh, Lukas Retschmeier </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2503.11897">PREAMBLE: Private and Efficient Aggregation of Block Sparse Vectors and Applications</a> <br> <i> Hilal Asi, Vitaly Feldman, Hannah Keller, Guy Rothblum, Kunal Talwar </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2505.05347">InfTDA: A Simple TopDown Mechanism for Hierarchical Differentially Private Counting Queries</a> <br> <i> Fabrizio Boninsegna </i> </li> <br>
<li> Bridging Privacy and Accuracy: Asymptotically Negligible Noise by Exponential Mechanism <br> <i> Young Hyun Cho, Yu Wei Chen, Jordan Awan </i> </li> <br>
<li> Schedule Indistinguishability: Applying Differential Privacy Models to Protect Runtime Timing Behaviors <br> <i> Maryam Ghorbanvirdi, Sibin Mohan </i> </li> <br>
<li> Efficient Optimization and Asymptotic Analysis of the Generalized Gaussian Mechanism for Differential Privacy <br> <i> Rachel Cummings, Soufiane Fafe </i> </li> <br>
</ul>

<a name="poster3"></a>
<p><b>Poster Session 3</b></p>
<ul>
<li> <a href="https://arxiv.org/pdf/2501.18121">Optimal Survey Design for Private Mean Estimation</a> <br> <i> Yu-Wei Chen, Jordan Awan, Raghu Pasupathy </i> </li> <br>
<li> Leveraging Per-Instance Privacy for Machine Unlearning <br> <i> Nazanin Mohammadi Sepahvand, Anvith Thudi, Berivan Isik, Ashmita Bhattacharyya, Nicolas Papernot, Eleni Triantafillou, Daniel M. Roy, Gintare Karolina Dziugaite </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2503.07199">How Well Can Differential Privacy Be Audited in One Run?</a> <br> <i> Amit Keinan, Moshe Shenfeld, Katrina Ligett </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2502.02709">Enforcing Demographic Coherence: A Harms Aware Framework for Reasoning about Private Data Release</a> <br> <i> Mark Bun, Marco Carmosino, Palak Jain, Gabriel Kaptchuk, Satchit Sivakumar </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2410.17566">Differentially Private Learning Needs Better Model Initialization and Self-Distillation</a> <br> <i> Ivoline Ngong, Joseph P. Near, Niloofar Mireshghallah </i> </li> <br>
<li> Sample-Efficient Private Learning of Mixtures of Gaussians <br> <i> Mahbod Majid, Hasan Ashtiani, Shyam Narayanan </i> </li> <br>
<li> Differentially Private Sequential Data Synthesis with Structured State Space Model and Diffusion Model <br> <i> Tomoya Matsumoto, Takayuki Miura, Toshiki Shibahara, Masanobu Kii, Kazuki Iwahana, Osamu Saisho, Shingo Okamura </i> </li> <br>
<li> Partial-Information Fragment Inference from LLMs <br> <i> Lucas Rosenblatt, Bin Han, Robert Wolfe, Bill Howe </i> </li> <br>
<li> <a href="https://www.arxiv.org/abs/2502.13314v4">Debiasing Functions of Private Statistics in Postprocessing</a> <br> <i> Flavio Calmon, Elbert Du, Cynthia Dwork, Brian Finley, Grigory Franguridi </i> </li> <br>
<li> <a href="https://arxiv.org/pdf/2405.20405">Private Mean Estimation with Person-Level Differential Privacy</a> <br> <i> Sushant Agarwal, Gautam Kamath, Mahbod Majid, Argyris Mouzakis, Rose Silver, Jonathan Ullman </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2503.14709">Better Private Distribution Testing by Leveraging Unverified Auxiliary Data</a> <br> <i> Maryam Aliakbarpour, Arnav Burudgunte, Clément Canonne, Ronitt Rubinfeld </i> </li> <br>
<li> <a href="https://arxiv.org/pdf/2502.14087">Learning from End User Data with Shuffled Differential Privacy over Kernel Densities</a> <br> <i> Tal Wagner </i> </li> <br>
<li> Two Algorithms for Prediction with Expert Advice under Local Differential Privacy <br> <i> Ben Jacobsen, Kassem Fawaz </i> </li> <br>
<li> Efficient, Black-box Inference Attacks on Shared Representations in Multitask Learning <br> <i> John Abascal, Nicolás Berrios, Alina Oprea, Jonathan Ullman, Adam Smith, Matthew Jagielski </i> </li> <br>
<li> Trade-offs in Data Memorization via Strong Data Processing Inequalities <br> <i> Vitaly Feldman, Guy Kornowski, Xin Lyu, Xin Lyu </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2412.10357">The Correlated Gaussian Sparse Histogram Mechanism</a> <br> <i> Christian Janos Lebeda, Lukas Retschmeier </i> </li> <br>
<li> <a href="pdf/sekiguchi.pdf">Label Differential Privacy can Release Unexpected Leakage</a> <br> <i> Hinata Sekiguchi, Shun Takagi, Satoshi Hasegawa, Marin Matsumoto, Masato Oguchi </i> </li> <br>
<li> Differentially Private Distributed Mean Estimation with Constrained User Correlations <br> <i> Sajani Vithana, Viveck R. Cadambe, Flavio P. Calmon, Haewon Jeong </i> </li> <br>
<li> <a href="https://johannaduengler.github.io/TPDP_2025_submission.pdf">An Iterative Algorithm for Differentially Private $k$-PCA with adaptive noise</a> <br> <i> Johanna Düngler, Amartya Sanyal </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2410.18404">Enhancing Feature-Specific Data Protection via Bayesian Coordinate Differential Privacy</a> <br> <i> Syomantak Chaudhuri, Maryam Aliakbarpour, Thomas A. Courtade, Alireza Fallah, Michael I. Jordan </i> </li> <br>
<li> Bridging Privacy and Accuracy: Asymptotically Negligible Noise by Exponential Mechanism <br> <i> Young Hyun Cho, Yu Wei Chen, Jordan Awan </i> </li> <br>
<li> <a href="pdf/mahpud.pdf">A Private Approximation of the 2nd-Moment Matrix of Any Subsamplable Input</a> <br> <i> Bar Mahpud, Or Sheffet </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2403.12213">Private graphon estimation via sum-of-squares</a> <br> <i> Hongjie Chen, Jingqiu Ding, Tommaso d'Orsi, Yiding Hua, Chih-Hung Liu, David Steurer </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2505.08557">Online Learning and Unlearning</a> <br> <i> Yaxi Hu, Bernhard Schölkopf, Amartya Sanyal </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2409.17623">Fully Dynamic Graph Algorithms with Edge Differential Privacy</a> <br> <i> Sofya Raskhodnikova, Teresa Anna Steiner </i> </li> <br>
<li> Differentially Private Shortest Distances in Continual Release Model <br> <i> Rachel Cummings, Tamalika Mukherjee, Jalaj Upadhyay, Hantao Yu, Zongrui Zou, Hantao Yu </i> </li> <br>
<li> Differentially Private Learning Beyond the Classical Dimensionality Regime <br> <i> Cynthia Dwork, Pranay Tankala, Linjun Zhang </i> </li> <br>
<li> <a href="https://arxiv.org/abs/2501.18914">Scaling Laws for Differentially Private Language Models</a> <br> <i> Ryan McKenna, Yangsibo Huang, Amer Sinha, Borja Balle, Zachary Charles, Christopher A. Choquette-Choo, Badih Ghazi, George Kaissis, Ravi Kumar, Ruibo Liu, Da Yu, Chiyuan Zhang </i> </li> <br>
<li> Faster Algorithms for Person-Level Differentially Private Stochastic Convex Optimization <br> <i> Andrew Lowy, Daogao Liu, Hilal Asi </i> </li> <br>
<li> dpmm: Differentially Private Marginal Models, a Library for Synthetic Tabular Data Generation <br> <i> Sofiane Mahiou, Amir Dizche, Reza Nazari, Xinmin Wu, Ralph Abbey, Jorge Silva, Georgi Ganev, Georgi Ganev </i> </li> <br>
<li> Differential Privacy and the Survey Data Pipeline <br> <i> Joerg Drechsler, James Bailie </i> </li> <br>
<li> <a href="pdf/fu.pdf">Towards Vertically Distributed Differentially Private Synthetic Data Generation</a> <br> <i> Yucheng Fu, Tianyao Gu, Elaine Shi, Tianhao Wang </i> </li> <br>
<li> <a href="pdf/imola.pdf">Private Quantile Estimation in the Two-Server Model</a> <br> <i> Anders Aamand, Fabrizio Boninsegna, Jacob Imola, Hannah Keller, Rasmus Pagh, Amrita Roy Chowdhury </i> </li> <br>
<li> Fast Adaptive Private Query Answering for Large Data Domains <br> <i> Brett Mullins, Miguel Fuentes, Yingtai Xiao, Daniel Kifer, Cameron Musco, Daniel Sheldon </i> </li> <br>
</ul>

</div>

<div class="content box">

<h2>Call for Papers</h2>

<p>
Differential privacy (DP) is the leading framework for data analysis with rigorous privacy guarantees. In the last two decades, it has transitioned from the realm of pure theory to large scale, real world deployments.
</p>
<p>
Differential privacy is an inherently interdisciplinary field, drawing researchers from a variety of academic communities including machine learning, statistics, security, theoretical computer science, databases, and law. The combined effort across a broad spectrum of computer science is essential for differential privacy to realize its full potential.  To this end, this workshop aims to stimulate discussion among participants about both the state-of-the-art in differential privacy and the future challenges that must be addressed to make differential privacy more practical.
</p>

<p>
Specific topics of interest for the workshop include (but are not limited to):
</p>
<ul>
<li>Theory of DP</li>
<li>DP and security</li>
<li>Privacy preserving machine learning</li>
<li>DP and statistics</li>
<li>DP and data analysis</li>
<li>Trade-offs between privacy protection and analytic utility</li>
<li>DP and surveys</li>
<li>Programming languages for DP</li>
<li>Relaxations of DP</li>
<li>Relation to other privacy notions and methods</li>
<li>Experimental studies using DP</li>
<li>DP implementations</li>
<li>DP and policy making</li>
<li>Applications of DP</li>
<li>Reconstruction attacks and memorization</li>
</ul>


<p>
<b>Submissions:</b> Authors are invited to submit a short abstract of new work or work published since June 2024 (the most recent TPDP submission deadline). Submissions must be 4 pages maximum, not including references. Submissions may also include appendices, but these are only read at reviewer's discretion. There is no prescribed style file, but authors should ensure a minimum of 1-inch margins and 10pt font. Submissions are not anonymized, and should include author names and affiliations.
</p>

<p>
Submissions will undergo a lightweight review process and will be judged on originality, relevance, interest, and clarity. Based on the volume of submissions to TPDP 2024 and the workshop's capacity constraints, we expect that the review process will be somewhat more competitive than in years past. Accepted abstracts will be presented at the workshop either as a talk or a poster.
</p>

<p>
The workshop will not have formal proceedings and is not intended to preclude later publication at another venue. In-person attendance is encouraged, though authors of accepted abstracts who cannot attend in person will be invited to submit a short video to be linked on the TPDP website.
</p>

<p>
Selected papers from the workshop will be invited to submit a full version of their work for publication in a <a href="https://journalprivacyconfidentiality.org/index.php/jpc/tpdp">special issue</a> of the <a href="https://journalprivacyconfidentiality.org/index.php/jpc">Journal of Privacy and Confidentiality</a>.
</p>

</div>


<div class="side box">
  <h2>Important Dates</h2>
  <dl>
    <dt>Abstract Submission</dt>
    <dd>March 20, 2025 (AoE)</dd>
    <dt>Notification</dt>
    <dd>May 1, 2025</dd>
    <dt>Workshop</dt>
    <dd>June 2-3, 2025</dd>
  </dl>
</div>

<div class="side box">
<h2 id="corpsponsor">Corporate Sponsors</h2>

<p>
We are very grateful to our sponsors whose generosity has been critical to the continued success of the workshop. For information about sponsorship opportunities, please contact us at <a href="mailto:tpdp.chairs@gmail.com">tpdp.chairs@gmail.com</a>.
</p>

<h2 class="small-heading">Workshop Host</h2>
<p align="center"><img src="google_logo.png" alt="Google logo" width="180"></p>

<h2 class="small-heading">Platinum Tier Sponsors</h2>
<p align="center"><img src="apple_logo.png" alt="Apple logo" width="80"></p>
<p align="center"><img src="google_logo.png" alt="Google logo" width="180"></p>

<h2 class="small-heading">Gold Tier Sponsors</h2>
<p align="center"><img src="capital_one_logo.svg" alt="Capital One logo" width="200"></p>

<h2 class="small-heading">Silver Tier Sponsors</h2>
<p align="center">  <img src="dpella_logo.jpg" alt="DPella logo" width="150"></p>

</div>

<div class="side box">
  <h2>Submission website</h2>
<dl>
<a href="https://tpdp25.cs.uchicago.edu/">https://tpdp25.cs.uchicago.edu</a>
  </dl>
<p>
For concerns regarding submissions, please contact <a href="mailto:tpdp.chairs@gmail.com">tpdp.chairs@gmail.com</a>
</p>
</div>


<div class="side box">
  <h2>Organizing and Program Committee</h2>
  <ul>
    <li class="pc">
      <a href="https://www.uvm.edu/~jnear/">Joe Near</a> (co-chair)<br>
      <i>University of Vermont</i>
    </li>

    <li class="pc">
      <a href="https://jksarathy.github.io/">Jayshree Sarathy</a> (co-chair)<br>
      <i>Northeastern University</i>
    </li>

<li class="pc">
Adam Sealfon<br>
<i>Google</i>
</li>

<li class="pc">
Adam Smith<br>
<i>Boston University</i>
</li>

<li class="pc">
Ajinkya K Mulay<br>
<i>Meta</i>
</li>

<li class="pc">
Alejandro Russo<br>
<i>Chalmers University of Technology, Göteborg University, DPella AB</i>
</li>

<li class="pc">
Alessandro Epasto<br>
<i>Google Research</i>
</li>

<li class="pc">
Alexandra Wood<br>
<i>Berkman Klein Center for Internet & Society at Harvard University</i>
</li>

<li class="pc">
Amartya Sanyal<br>
<i>University of Copenhagen </i>
</li>

<li class="pc">
Amin Rahimian <br>
<i>University of Pittsburgh </i>
</li>

<li class="pc">
Anand Sarwate<br>
<i>Rutgers University</i>
</li>

<li class="pc">
Andrew Lowy<br>
<i>University of Wisconsin-Madison</i>
</li>

<li class="pc">
Anish<br>
<i>UC Berkeley Undergrad</i>
</li>

<li class="pc">
Anne-Sophie Charest<br>
<i>Université Laval</i>
</li>

<li class="pc">
Antti Koskela<br>
<i>Nokia Bell Labs</i>
</li>

<li class="pc">
Anupama Nandi<br>
<i>Yale University</i>
</li>

<li class="pc">
Arun Ganesh<br>
<i>Google</i>
</li>

<li class="pc">
Ayelet Gordon-Tapiero <br>
<i>Hebrew University </i>
</li>

<li class="pc">
Christian Janos Lebeda<br>
<i>Inria</i>
</li>

<li class="pc">
Christine Task<br>
<i>Knexus</i>
</li>

<li class="pc">
Damien Desfontaines<br>
<i>Tumult Labs</i>
</li>

<li class="pc">
Danfeng Zhang<br>
<i>Duke University</i>
</li>

<li class="pc">
Daogao Liu<br>
<i>University of Washington</i>
</li>

<li class="pc">
Dima Usynin<br>
<i>TUM, Oblivious AI</i>
</li>

<li class="pc">
Dung Nguyen<br>
<i>University of Virginia</i>
</li>

<li class="pc">
Eli Chien<br>
<i>Georgia Institute of Technology</i>
</li>

<li class="pc">
Enayat Ullah<br>
<i>Meta</i>
</li>

<li class="pc">
Gautam Kamath<br>
<i>University of Waterloo</i>
</li>

<li class="pc">
Gavin Brown<br>
<i>University of Washington</i>
</li>

<li class="pc">
Hao Wu<br>
<i>University of Waterloo</i>
</li>

<li class="pc">
Hilal Asi<br>
<i>Apple</i>
</li>

<li class="pc">
Hilla Schefler<br>
<i>The Technion</i>
</li>

<li class="pc">
Ivoline Ngong<br>
<i>University of Vermont</i>
</li>

<li class="pc">
Jalaj Upadhyay<br>
<i>Rutgers University</i>
</li>

<li class="pc">
Jatan Loya<br>
<i>Google</i>
</li>

<li class="pc">
Jiayuan Ye<br>
<i>National University of Singapore</i>
</li>

<li class="pc">
Joann Chen<br>
<i>San Diego State University </i>
</li>

<li class="pc">
Joerg Drechsler<br>
<i>Institute for Employment Research and LMU Munich</i>
</li>

<li class="pc">
Johes Bater<br>
<i>Tufts University</i>
</li>

<li class="pc">
Jonathan Ullman<br>
<i>Northeastern University</i>
</li>

<li class="pc">
Juba Ziani<br>
<i>Georgia Tech</i>
</li>

<li class="pc">
Kunal Talwar<br>
<i>Apple</i>
</li>

<li class="pc">
Linjun Zhang<br>
<i>Rutgers University</i>
</li>

<li class="pc">
Linsheng Liu<br>
<i>George Washington University</i>
</li>

<li class="pc">
Lucas Rosenblatt<br>
<i>NYU</i>
</li>

<li class="pc">
Ludmila Glinskih<br>
<i>Google</i>
</li>

<li class="pc">
Lydia Zakynthinou<br>
<i>UC Berkeley</i>
</li>

<li class="pc">
Mahdi haghifam<br>
<i>Northeastern University</i>
</li>

<li class="pc">
Mark Bun<br>
<i>Boston University</i>
</li>

<li class="pc">
Maryam Aliakbarpour<br>
<i>Rice University</i>
</li>

<li class="pc">
Matthew Jagielski<br>
<i>Google DeepMind</i>
</li>

<li class="pc">
Matthew Joseph<br>
<i>Google Research</i>
</li>

<li class="pc">
Mayana Pereira<br>
<i>Capital One</i>
</li>

<li class="pc">
Moshe Shenfeld<br>
<i>Hebrew University of Jerusalem</i>
</li>

<li class="pc">
Olya Ohrimenko<br>
<i>The University of Melbourne</i>
</li>

<li class="pc">
Om Thakkar<br>
<i>OpenAI</i>
</li>

<li class="pc">
Or Sheffet<br>
<i>Bar Ilan University </i>
</li>

<li class="pc">
Palak Jain<br>
<i>Boston University </i>
</li>

<li class="pc">
Peter Kirouz<br>
<i>Google</i>
</li>

<li class="pc">
Pierre Tholoniat <br>
<i>Columbia University</i>
</li>

<li class="pc">
Prajjwal Gupta<br>
<i>Cloudflare Research</i>
</li>

<li class="pc">
Priyanka Nanayakkara<br>
<i>Harvard University</i>
</li>

<li class="pc">
Rachel Cummings<br>
<i>Columbia University</i>
</li>

<li class="pc">
Rasmus Pagh<br>
<i>University of Copenhagen</i>
</li>

<li class="pc">
Roi Livni<br>
<i>Tel Aviv University</i>
</li>

<li class="pc">
Ryan McKenna<br>
<i>Google</i>
</li>

<li class="pc">
Saeyoung Rho<br>
<i>Columbia University</i>
</li>

<li class="pc">
Sajani Vithana<br>
<i>Harvard University</i>
</li>

<li class="pc">
Sam Haney<br>
<i>Tumult Labs</i>
</li>

<li class="pc">
Samuel Haney<br>
<i>Tumult Labs</i>
</li>

<li class="pc">
Satchit Sivakumar<br>
<i>Boston University</i>
</li>

<li class="pc">
Shengyuan Hu<br>
<i>Carnegie Mellon University</i>
</li>

<li class="pc">
Shlomi Hod<br>
<i>Boston University</i>
</li>

<li class="pc">
Shubhankar Mohapatra<br>
<i>University of Waterloo</i>
</li>

<li class="pc">
Sofya Raskhodnikova<br>
<i>Boston University</i>
</li>

<li class="pc">
Tamalika Mukherjee <br>
<i>Columbia university </i>
</li>

<li class="pc">
Thomas Humphries<br>
<i>University of Waterloo</i>
</li>

<li class="pc">
Thomas Steinke<br>
<i>Google DeepMind</i>
</li>

<li class="pc">
Tianhao Wang<br>
<i>University of Virginia</i>
</li>

<li class="pc">
Tudor Cebere<br>
<i>Inria</i>
</li>

<li class="pc">
Uri Stemmer<br>
<i>Tel Aviv University</i>
</li>

<li class="pc">
Vitaly Feldman<br>
<i>Apple</i>
</li>

<li class="pc">
Viveck Cadambe<br>
<i>Georgia Tech</i>
</li>

<li class="pc">
Xingyu Zhou<br>
<i>Wayne State University</i>
</li>

<li class="pc">
Yaodong Yu<br>
<i>OpenAI</i>
</li>

<li class="pc">
Yingtai Xiao<br>
<i>TikTok </i>
</li>

<li class="pc">
Youssef Allouah<br>
<i>EPFL</i>
</li>

<li class="pc">
Yuzheng Hu<br>
<i>University of Illinois Urbana-Champaign</i>
</li>

<li class="pc">
Yves-Alexandre de Montjoye<br>
<i>Imperial College London</i>
</li>

<li class="pc">
Zeyu Ding<br>
<i>Binghamton University</i>
</li>
  </ul>
</div>


<div id="bg_descr">
</div>

<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>

</body></html>
